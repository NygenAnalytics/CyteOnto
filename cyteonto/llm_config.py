# cyteonto/llm_config.py

from typing import Any, Literal, TypeAlias

from pydantic import BaseModel, Field, ValidationError
from pydantic_ai import (
    Agent,
    CallToolsNode,
    ModelHTTPError,
    UnexpectedModelBehavior,
    UsageLimitExceeded,
)
from pydantic_ai.messages import ModelMessage, ToolCallPart
from pydantic_ai.usage import RunUsage, UsageLimits
from pydantic_graph import End
from tenacity import (
    RetryCallState,
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_chain,
    wait_fixed,
    wait_random,
)

from .logger_config import logger


class AgentConfig:
    TOOL_DEFAULT_SETTINGS: dict[str, str | int] = {
        "retries": 2,
        "strict": False,
    }
    AGENT_DEFAULT_USAGE_LIMITS: UsageLimits = UsageLimits(
        request_limit=50,
        input_tokens_limit=20_000,
    )
    MAX_TOOL_CALLS: int = 3
    MAX_CONCURRENT_DESCRIPTIONS: int = 10


AGENT_CONFIG = AgentConfig()
EMBDProvider: TypeAlias = Literal["deepinfra", "ollama", "openai", "google"]


class EMBDModelConfig(BaseModel):
    """
    Configuration for embedding models.
    Not in use for now.
    """

    provider: EMBDProvider = Field(
        default="deepinfra", description="The embedding provider to use."
    )
    model: str = Field(
        default="Qwen/Qwen3-Embedding-8B",
        description="The name of the embedding model to use (as per the provider).",
    )
    apiKey: str = Field(
        default="DEEPINFRA_TOKEN", description="The API key for the embedding model"
    )
    modelSettings: dict[str, Any] | None = Field(
        default=None, description="Extra body for the embedding model"
    )
    maxConcEmbed: int = Field(
        default=10,
        description="Maximum concurrent embedding requests to avoid rate limits.",
    )


class AgentUsage(BaseModel):
    agent_name: str = Field(
        description="Name of the agent (e.g., 'annotator', 'reviewer')"
    )
    model_name: str = Field(default="", description="Name of the LLM model used")
    requests: int = Field(
        default=0, description="Number of requests made by this agent"
    )
    request_tokens: int = Field(
        default=0, description="Input tokens consumed by this agent"
    )
    response_tokens: int = Field(
        default=0, description="Output tokens generated by this agent"
    )
    total_tokens: int = Field(
        default=0, description="Total tokens (input + output) for this agent"
    )
    runtime_seconds: float = Field(
        default=0.0, description="Runtime in seconds for this agent"
    )
    tool_usage: dict[str, int] = Field(
        default_factory=dict, description="Tool usage counts for this agent"
    )

    def update(
        self,
        model_name: str,
        llm_usage: RunUsage,
        tool_usage: dict[str, int] | None = None,
    ) -> None:
        self.model_name = model_name
        self.requests += llm_usage.requests
        self.request_tokens += llm_usage.input_tokens or 0
        self.response_tokens += llm_usage.output_tokens or 0
        self.total_tokens += llm_usage.total_tokens or 0
        if tool_usage:
            for tool, count in tool_usage.items():
                self.tool_usage[tool] = self.tool_usage.get(tool, 0) + count


def get_embd_param(embd_model_config: EMBDModelConfig) -> dict:
    provider_url_map = {
        "deepinfra": "https://api.deepinfra.com/v1/openai",
        "ollama": "http://localhost:11434/v1/",
        "openai": "https://api.openai.com/v1/",
        "google": "https://generativelanguage.googleapis.com/v1beta/models/gemini-embedding-001:embedContent",
    }
    provider_headers = {
        "deepinfra": {
            "Authorization": f"Bearer {embd_model_config.apiKey}",
            "Content-Type": "application/json",
        },
        "ollama": {},
        "openai": {
            "Authorization": f"Bearer {embd_model_config.apiKey}",
            "Content-Type": "application/json",
        },
        "google": {
            "x-goog-api-key": f"{embd_model_config.apiKey}",
            "Content-Type": "application/json",
        },
    }
    if embd_model_config.provider not in provider_url_map:
        raise ValueError(f"Unsupported provider: {embd_model_config.provider}")
    return {
        "url": provider_url_map[embd_model_config.provider],
        "headers": provider_headers[embd_model_config.provider],
    }


def get_embd_results(results: dict, embd_model_config: EMBDModelConfig) -> list[float]:
    provider = embd_model_config.provider
    if provider == "google":
        if results.get("embeddings") and len(results["embeddings"]) > 0:
            return results["embeddings"]["values"]
        else:
            return []
    else:
        if results.get("data") and len(results["data"]) > 0:
            return results["data"][0]["embedding"]
        else:
            return []


def get_tool_counts(graph_node: Any) -> dict[str, int]:
    tools = {}
    if isinstance(graph_node, CallToolsNode):
        for tool in graph_node.model_response.parts:
            if isinstance(tool, ToolCallPart):
                if tool.tool_name not in tools:
                    tools[tool.tool_name] = 0
                tools[tool.tool_name] += 1
    return tools


def update_tool_usage(graph_node: Any, tool_usage: dict[str, int]) -> None:
    """Update tool usage counter for a given node."""
    for tool, count in get_tool_counts(graph_node).items():
        tool_usage[tool] = tool_usage.get(tool, 0) + count


async def agent_run[DepsType, ResultType](
    agent: Agent[DepsType, ResultType],
    user_prompt: str,
    deps_data: DepsType,
    agent_usage: AgentUsage,
    messages: list[ModelMessage] = [],
    message_history: list[ModelMessage] = [],
) -> ResultType:
    def _blank_output() -> ResultType:
        ret_val: ResultType = agent.output_type.get_blank()  # type: ignore
        return ret_val

    def _log_retry_failure(retry_state: RetryCallState) -> None:
        error = (
            retry_state.outcome.exception() if retry_state.outcome else "Unknown error"
        )
        logger.error(
            f"Agent '{agent.name}' failed after {retry_state.attempt_number} attempts. "
            f"Last error: {error}"
        )

    def _retry_error_callback(retry_state: RetryCallState) -> ResultType:
        raw_exc = retry_state.outcome.exception() if retry_state.outcome else None
        exc: Exception | None = raw_exc if isinstance(raw_exc, Exception) else None
        logger.error(exc)
        return _blank_output()

    @retry(
        stop=stop_after_attempt(3),
        wait=wait_chain(
            wait_fixed(5),  # wait fixed for 5 seconds
            # wait a 5+x random seconds twice
            wait_fixed(5) + wait_random(0, 5),
            wait_fixed(5) + wait_random(0, 10),
        ),
        retry=retry_if_exception_type(
            (ModelHTTPError, UnexpectedModelBehavior, ValidationError)
        ),
        after=_log_retry_failure,
        retry_error_callback=_retry_error_callback,
    )
    async def run() -> ResultType:
        usage = RunUsage()
        tool_usage: dict[str, int] = {}
        try:
            async with agent.iter(
                user_prompt=user_prompt,
                deps=deps_data,
                usage=usage,
                usage_limits=AGENT_CONFIG.AGENT_DEFAULT_USAGE_LIMITS,
                message_history=message_history,
            ) as agent_run:
                node = agent_run.next_node

                while not isinstance(node, End):
                    new_node = await agent_run.next(node)
                    update_tool_usage(node, tool_usage)
                    node = new_node

                if agent_run.result is None:
                    logger.warning(f"{agent.name} produced no result")
                    return _blank_output()

        except (ModelHTTPError, UnexpectedModelBehavior, ValidationError) as e:
            raise e
        except UsageLimitExceeded as e:
            logger.error(f"Agent '{agent.name}' failed due to usage limit: {e}")
            return _blank_output()
        except Exception as e:
            logger.error(
                f"Agent '{agent.name}' caught unexpected error: {e.__class__.__name__}: {str(e)}"
            )
            return _blank_output()

        agent_usage.update(
            str(agent.model.model_name),  # type: ignore
            usage,
            tool_usage,
        )
        messages.extend(agent_run.result.all_messages())
        return agent_run.result.output

    return await run()
